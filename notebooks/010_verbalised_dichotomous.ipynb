{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jinja2 import Template\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Openrouter constants\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENTROUTER_BASE_URL = 'https://openrouter.ai/api/v1'\n",
    "\n",
    "\n",
    "## Experiment constants\n",
    "EXPERIMENT_NAME = 'exp1.0_dich'  # baseline\n",
    "MODEL_NAME = 'openai/gpt-4o-mini'\n",
    "# MODEL_NAME = 'openai/gpt-4'\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "RANDOM_SEED = 20250402\n",
    "\n",
    "N_ARMS = 5\n",
    "DELTA = 0.2\n",
    "N_TRIALS = 10\n",
    "\n",
    "\n",
    "ARM_NAME_TO_IDX = {'blue': 0, 'green': 1, 'red': 2, 'yellow': 3, 'purple': 4}\n",
    "ARM_IDX_TO_NAME = {v: k for k, v in ARM_NAME_TO_IDX.items()}\n",
    "\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "EXPERIMENT_DIR = DATA_DIR / EXPERIMENT_NAME\n",
    "EXPERIMENT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def bootstrap() -> None:\n",
    "    # logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\\\n",
    "You are a bandit algorithm in a room with {{n_arms}} buttons labeled blue, green, red, yellow, purple. \\\n",
    "Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means \\\n",
    "for the buttons could be different. For each button, when you press it, you will get a reward \\\n",
    "that is sampled from the button's associated distribution. You have {{n_trials}} time steps and, on each \\\n",
    "time step, you can choose any button and receive the reward. Your goal is to maximize the total \\\n",
    "reward over the {{n_trials}} time steps.\n",
    "\n",
    "At each time step, I will show you a summary of your past choices and rewards. Then you must \\\n",
    "make the next choice, which must be exactly one of blue, green, red, yellow, purple. Let's think \\\n",
    "step by step to make sure we make a good choice. You must provide your final answer within the \\\n",
    "tags <Answer>COLOR</Answer> where COLOR is one of blue, green, red, yellow, purple.\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT_TEMPLATE = Template(SYSTEM_PROMPT_TEMPLATE)\n",
    "\n",
    "\n",
    "USER_PROMPT_TEMLATE = \"\"\"\\\n",
    "So far you have played {{n_trials}} times with your past choices and rewards summarized\n",
    "as follows:\n",
    "- blue button: pressed {{blue_n_occur}} times {%- if blue_n_occur != 0 %} with a {{blue_avg_reward}} average reward \\\n",
    "{% endif %}\n",
    "- green button: pressed {{green_n_occur}} times {%- if green_n_occur != 0 %} with a {{green_avg_reward}} average reward \\\n",
    "{% endif %}\n",
    "- red button: pressed {{red_n_occur}} times {%- if red_n_occur != 0 %} with a {{red_avg_reward}} average reward \\\n",
    "{% endif %}\n",
    "- yellow button: pressed {{yellow_n_occur}} times {%- if yellow_n_occur != 0 %} with a {{yellow_avg_reward}} average \\\n",
    "reward {% endif %}\n",
    "- purple button: pressed {{purple_n_occur}} times {%- if purple_n_occur != 0 %} with a {{purple_avg_reward}} average \\\n",
    "reward {% endif %}\n",
    "\n",
    "Which button will you choose next? Remember, YOU MUST provide your final answer within the tags \\\n",
    "<Answer>COLOR</Answer> where COLOR is one of blue, green, red, yellow, purple. Let's think step \\\n",
    "by step to make sure we make a good choice.\n",
    "\"\"\"\n",
    "USER_PROMPT_TEMLATE = Template(USER_PROMPT_TEMLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far you have played 10 times with your past choices and rewards summarized\n",
      "as follows:\n",
      "- blue button: pressed 4 times with a 0.4 average reward \n",
      "- green button: pressed 0 times\n",
      "- red button: pressed 3 times with a 0.1 average reward \n",
      "- yellow button: pressed 0 times\n",
      "- purple button: pressed 3 times with a 0.2 average reward \n",
      "\n",
      "Which button will you choose next? Remember, YOU MUST provide your final answer within the tags <Answer>COLOR</Answer> where COLOR is one of blue, green, red, yellow, purple. Let's think step by step to make sure we make a good choice.\n"
     ]
    }
   ],
   "source": [
    "example_prompt_inputs = {\n",
    "    'n_trials': 10,\n",
    "    'blue_n_occur': 4,\n",
    "    'blue_avg_reward': 0.4,\n",
    "    'green_n_occur': 0,\n",
    "    'green_avg_reward': 0,\n",
    "    'red_n_occur': 3,\n",
    "    'red_avg_reward': 0.1,\n",
    "    'yellow_n_occur': 0,\n",
    "    'yellow_avg_reward': 0,\n",
    "    'purple_n_occur': 3,\n",
    "    'purple_avg_reward': 0.2\n",
    "}\n",
    "print(USER_PROMPT_TEMLATE.render(**example_prompt_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, n_arms: int, delta: float) -> None:\n",
    "        self.n_arms = n_arms\n",
    "        self.delta = delta\n",
    "\n",
    "        means = [0.5 + self.delta / 2] + [0.5 - self.delta / 2] * (self.n_arms - 1)\n",
    "        np.random.shuffle(means)\n",
    "\n",
    "        self.means = means\n",
    "        self.best_arm = np.argmax(means)\n",
    "\n",
    "    def pull(self, arm: int) -> int:\n",
    "        assert 0 <= arm < self.n_arms, f\"Arm {arm} doesn't exist\"\n",
    "\n",
    "        p = self.means[arm]\n",
    "        reward = np.random.binomial(1, p=p)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client() -> OpenAI:\n",
    "    client = OpenAI(\n",
    "        base_url=OPENTROUTER_BASE_URL,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "def get_prediction(client: OpenAI, system_prompt: str, user_prompt: str, **kwargs) -> str | None:\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prediction = None\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,\n",
    "            **kwargs\n",
    "        )\n",
    "        prediction = completion.choices[0].message.content\n",
    "    except Exception as _:\n",
    "        logging.warning('Request failed')\n",
    " \n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Change in reward_transcription\n",
    "def build_prompt_inputs(trial: int, rewards: t.Dict[str, int], occurrences: t.Dict[str, int]) -> t.Dict[str, float]:\n",
    "    def average(key: str):\n",
    "        occ = occurrences[key]\n",
    "        total = rewards[key]\n",
    "        return total / occ if occ != 0 else 0.0\n",
    "    \n",
    "    def dichotomous_categorisation(average_reward: float):\n",
    "        sign = \"positive\" if average_reward > 0 else \"negative\"\n",
    "        \n",
    "        return sign\n",
    "    \n",
    "    prompt_inputs = {\n",
    "        'n_trials': trial,\n",
    "\n",
    "        'blue_n_occur': occurrences['blue'],\n",
    "        'blue_avg_reward': dichotomous_categorisation(average_reward=average('blue')),\n",
    "\n",
    "        'red_n_occur': occurrences['red'],\n",
    "        'red_avg_reward': dichotomous_categorisation(average_reward=average('red')),\n",
    "\n",
    "        'green_n_occur': occurrences['green'],\n",
    "        'green_avg_reward': dichotomous_categorisation(average_reward=average('green')),\n",
    "\n",
    "        'yellow_n_occur': occurrences['yellow'],\n",
    "        'yellow_avg_reward': dichotomous_categorisation(average_reward=average('yellow')),\n",
    "\n",
    "        'purple_n_occur': occurrences['purple'],\n",
    "        'purple_avg_reward': dichotomous_categorisation(average_reward=average('purple')),\n",
    "    }\n",
    "\n",
    "    return prompt_inputs\n",
    "\n",
    "\n",
    "def extract_arm_color(prediction: str) -> str | None:    \n",
    "    names = '|'.join(ARM_NAME_TO_IDX.keys())\n",
    "    pattern = f'<Answer>({names})</Answer>'\n",
    "    match = re.search(pattern, prediction)\n",
    "\n",
    "    color = None\n",
    "    if match:\n",
    "        color = match.group(1)\n",
    "\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_client()\n",
    "bandit = MultiArmedBandit(n_arms=N_ARMS, delta=DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.47s/it]\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "\n",
    "rewards = {k: 0 for k in ARM_NAME_TO_IDX.keys()}\n",
    "occurrences = {k: 0 for k in ARM_NAME_TO_IDX.keys()}\n",
    "\n",
    "for trial in tqdm(range(N_TRIALS), total=N_TRIALS):\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.render({'n_arms': N_ARMS, 'n_trials': N_TRIALS})\n",
    "\n",
    "    user_prompt_inputs = build_prompt_inputs(trial, rewards, occurrences)\n",
    "    user_prompt = USER_PROMPT_TEMLATE.render(user_prompt_inputs)\n",
    "    prediction = get_prediction(client, system_prompt, user_prompt)\n",
    "\n",
    "    arm_name = extract_arm_color(prediction)\n",
    "    arm_idx = ARM_NAME_TO_IDX[arm_name]\n",
    "\n",
    "    reward = bandit.pull(arm_idx)\n",
    "\n",
    "    occurrences[arm_name] += 1\n",
    "    rewards[arm_name] += reward\n",
    "\n",
    "    cumulative_reward = sum(rewards.values())\n",
    "    cumulative_reward_per_arm = {f'cumulative_reward_{k}': v for k, v in rewards.items()}\n",
    "    cumulative_occurrences_per_arm = {f'cumulative_occurrence_{k}': v for k, v in occurrences.items()}\n",
    "    stats.append({\n",
    "        'trial': trial,\n",
    "        'arm_name': arm_name,\n",
    "        'arm_idx': arm_idx,\n",
    "        'reward': reward,\n",
    "        'cumulative_reward': cumulative_reward,\n",
    "        'system_prompt': system_prompt,\n",
    "        'user_prompt': user_prompt,\n",
    "        'raw_prediction': prediction,\n",
    "        'best_arm': ARM_IDX_TO_NAME[bandit.best_arm],\n",
    "        **cumulative_reward_per_arm,\n",
    "        **cumulative_occurrences_per_arm\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "model_name = MODEL_NAME.replace('/', '_').replace(':', '_').replace('-', '_')\n",
    "version = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "filename = f'{EXPERIMENT_NAME}_{model_name}_arms-{N_ARMS}_delta-{DELTA}_trials-{N_TRIALS}_v{version}.csv'\n",
    "filepath = EXPERIMENT_DIR / filename\n",
    "\n",
    "df.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
